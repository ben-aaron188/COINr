---
title: "Normalisation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Normalisation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = F
)
```

```{r setup}
library(COINr)
```

ADD: qNormalise and remove spiel not related to COINr

Normalisation is the operation of bringing indicators onto comparable scales so that they can be aggregated more fairly. To see why this is necessary, consider aggregating GDP values (billions or trillions of dollars) with percentage tertiary graduates (tens of percent). Average values here would make no sense because one is on a completely different scale to the other.

# Approaches

## First: adjust direction

Indicators can either be positively or negatively related to the concept that you are trying to measure. For example, in an index of quality of life, median income would probably be a positive indicator. Prevalence of malnourishment would be a negative indicator (higher values should give lower scores in quality of life).

Accounting for these differences is considered part of the normalisation step. Indicators loaded into COINr should have a "Direction" column in the `IiMeta` input to `new_coin()`, which is 1 for positive indicators, and -1 for negative indicators. With this information, normalisation is a two step procedure:

1. Multiply the values of each indicator by their corresponding direction value (either 1 or -1).
2. Apply one of the normalisation methods described below.

It's that simple. COINr has this built in, so you don't need to do anything other than specify the directions, and the type of normalisation to apply.

## Linear transformations

Normalisation is relatively simple but there are still a number of different approaches which have different properties.

Perhaps the most straightforward and intuitive option (and therefore probably the most widely used) is called the *min-max* transformation. This is a simple linear function which rescales the indicator to have a minimum value $l$, a maximum value $u$, and consequently a range $u-l$, and is as follows:

$$ \tilde{x}_{\text{min}} = \frac{ x - x_{\text{min}} }{ x_{\text{max}} - x_{\text{min}} } \times (u-l) + l$$

where $\tilde{x}$ is the normalised indicator value. For example, if $l=0$ and $u=100$ this will rescale the indicator to lie exactly onto the interval $[0, 100]$. The transformation is linear because it does not change the *shape* of the distribution, it simply shrinks or expands it, and moves it.

A similar transformation is to take *z-scores*, which instead use the mean and standard deviation as reference points:

$$ \tilde{x}_{\text{min}} = \frac{ x - \mu_x }{ \sigma_x } \times a + b$$

where $\mu_x$ and $\sigma_x$ are the mean and standard deviation of $x$. The indicator is first re-scaled to have mean zero and standard deviation of one. Then it is scaled by a factor $a$ and moved by a distance $b$. This is very similar to the min-max transformation in that it can be reduced to multiplying by a factor and adding a constant, which is the definition of a linear transformation. However, the two approaches have different implications. One is that Z-scores will generally be less sensitive to outliers, because the standard deviation is less dependent on an outlying value than the minimum or maximum.

Following the min-max and z-score, the general linear transformation is defined as:

$$ \tilde{x} = \frac{ x - p }{ q } \times a + b$$

and it is fairly straightforward to see how z-scores and the min-max transformations are special cases of this.

## Nonlinear transformations

A simple nonlinear transformation is the rank transformation.

$$ \tilde{x} = \text{rank}(x)$$

where the ranks should be defined so that the lowest indicator value has a rank of 1, the second lowest a rank of 2, and so on. The rank transformation is attractive because it automatically eliminates any outliers. Therefore there would not usually be any need to treat the data previously. However, it converts detailed indicator scores to simple ranks, which might be too reductionist for some.

It's worth pointing out that there are different ways to rank values, because of how ties (units with the same score) are handled. To read about this, just call `?rank` in R.

Similar approaches to simple ranks include Borda scores, which are simply the ranks described above but minus 1 (so the lowest score is 0 instead of 1), and percentile ranks.

## Distances

Another approach is to use the distance of each score to some reference value. Possibilities here are the (normalised) distance to the maximum value of the indicator:

$$ \tilde{x} = 1 - \frac{\text{max}(x) - x}{\text{max}(x) - \text{min}(x)}$$

the fraction of the maximum value of the indicator:

$$ \tilde{x} = \frac{x}{\text{max}(x)}$$

the distance to a specified unit value in the indicator:

$$ \tilde{x} = 1 - \frac{x_u - x}{\text{max}(x) - \text{min}(x)}$$

where $x_u$ is the value of unit $u$ in the indicator. This is useful for benchmarking against a reference country, for example. Another possibility is to normalise against indicator targets. This approach is used for example in the [EU2020 Index](https://www.sciencedirect.com/science/article/pii/S2665972720300593), which used European targets on environment, education and employment issues (among others).

$$ \tilde{x} = \text{min} \left[1, \  1 - \frac{\text{targ}(x) - x}{\text{max}(x) - \text{min}(x)} \right]$$

where $\text{targ}(x)$ is the target for the indicator. In this case, any value that exceeds the target is set to 1, i.e. exceeding the target is counted the same as exactly meeting it. There is also an issue of what to use to scale the distance: here the range of the indicator is used, but one could also use $\text{targ}(x) - \text{min}(x)$ or perhaps some other range.

A similar approach, sometimes called the "goalposts" or "distance to frontier" method, normalises the indicator by specified upper and lower bounds. This may be used, for example, when there are clear boundaries to the indicator (e.g. [Likert scores](https://en.wikipedia.org/wiki/Likert_scale)), or when the you wish to keep the boundaries fixed across possibly different years of data. The goalposts method looks like this:

$$ \tilde{x} = \text{max} \left(0, \ \text{min} \left[1, \  \frac{x - x_L}{x_U - x_L} \right] \right)$$

where $x_U$ and $x_L$ are the upper and lower bounds, respectively, for the indicator. Any values that are below the lower bound are assigned a score of zero, and any above the upper bound are assigned a score of one. This approach is used by the [European Skills Index](https://www.cedefop.europa.eu/en/events-and-projects/projects/european-skills-index-esi), for example.

# Normalisation in COINr

The normalisation function in COINr is imaginatively named `normalise()`. It has the following main features:

* A wide range of normalisation methods, including the possibility to pass custom functions
* Customisable parameters for normalisation
* Possibility to specify detailed individual treatment for each indicator

As of COINr v8, `normalise()` is a generic function with methods for different classes. This means that `normalise()` can be called on coins, but also on data frames, numeric vectors and purses (collections of coins).

## Coins

The `normalise()` method for coins follows the familiar format: you have to specify:

* `x` the coin
* `default_specs` default specifications to apply to all indicators
* `indiv_specs` individual specifications to override `default_specs` for specific columns
* `directions` a data frame specifying directions - this overrides the directions in `iMeta` if specified
* `out2` whether to output an updated coin or simply a data frame

Let's begin with a simple example. We build the example coin and normalise the raw data.

```{r}
# build example coin
coin <- build_example_coin(up_to = "new_coin")

# normalise the raw data set
coin <- Normalise(coin, dset = "Raw")
```

We can compare the raw and un-normalised indicators side by side.

*PLACEHOLDER: scatter plot and side by side dist plots of indicators in ggplot2.*

This plot also illustrates the linear nature of the min-max transformation.

The default normalisation uses the min-max approach, scaling indicators onto the $[0, 100]$ interval. But we can change the normalisation type and its parameters using the `default_specs` argument.

```{r}
coin <- Normalise(coin, dset = "Raw",
                   default_specs = list(f_n = "n_zscore",
                                        f_n_para = list(c(10,2))))
```

Again, let's plot an example of the result:

*PLACEHOLDER: scatter plot and side by side dist plots of indicators in ggplot2.*

Notice the syntax of `default_specs`. If specified, it takes entries `f_n` (the name of the function to apply to each column) and `f_n_para` (any further arguments to `f_n`, not including `x`). Importantly, `f_n_para` *must* be specified as a list, even if it only contains one parameter.

Since `f_n` points to a function name, any function can be passed to `normalise()` as long as it is available in the namespace. To illustrate, consider an example where we want to categorise into discrete bins. We can use base R's `cut()` function for this purpose. We simply need to specify the number of bins. We could directly call `cut()`, but for clarity we will create a simple wrapper function around it, then pass that function to `normalise()`.

```{r}
# wrapper function
f_bin <- function(x, nbins){
  cut(x, breaks = nbins, labels = FALSE)
}

# pass wrapper to normalise, specify 5 bins
coin <- Normalise(coin, dset = "Raw",
                   default_specs = list(f_n = "f_bin",
                                        f_n_para = list(nbins = 5)))

get_dset(coin, "Normalised")[1:5] |>
  head(5)
```

Generally, the requirements of a function to be passed to `normalise()` are that its first argument should be `x`, a numeric vector, and it should return a numeric vector of the same length as `x`. It should also be able to handle `NA`s. Any further arguments can be passed via the `f_n_para` entry.

By default, the directions are taken from the coin. These will have been specified as the `Direction` column of `iMeta` when constructing a coin with `new_coin()`. However, you can specify different directions using the `directions` argument of `normalise()`: in this case you need to specify a data frame with two columns: `iCode` (with an entry for each indicator code found in the target data set) and `Direction` giving the direction as -1 or 1.

To show an example, we take the existing directions from the coin, modify them slightly, and then run the normalisation function again:

```{r}
# get directions from coin
directions <- coin$Meta$Ind[c("iCode", "Direction")]

head(directions, 10)
```

We'll change the direction of the "Goods" indicator and re-normalise:

```{r}
# change Goods to -1
directions$Direction[directions$iCode == "Goods"] <- -1

# re-run (using min max default)
coin <- Normalise(coin, dset = "Raw", directions = directions)
```

Finally let's explore how to specify different normalisation methods for different indicators. The `indiv_specs` argument takes a named list for each indicator, and will override the specifications in `default_specs`. If `indiv_specs` is specified, we only need to include sub-lists for indicators that differ from `default_specs`.

To illustrate, we can use a contrived example where we might want to apply min-max to all indicators except two. For those, we apply a rank transformation and distance to maximum approach. Note, that since the default of `default_specs` is min-max, we don't need to specify that at all here.

```{r}
# individual specifications:
# LPI - borda scores
# Flights - z-scores with mean 10 and sd 2
indiv_specs <- list(
  LPI = list(f_n = "n_borda"),
  Flights = list(f_n = "n_zscore",
                 f_n_para = list(m_sd = c(10, 2)))
)

# normalise
coin <- Normalise(coin, dset = "Raw", indiv_specs = indiv_specs)

# a quick look at the first three indicators
get_dset(coin, "Normalised")[1:4] |>
  head(10)
```

We can visualise the new ranges of the data.

*PLACEHOLDER: some kind of plots*

This example is meant to be illustrative of the functionality of `normalise()`, rather than being a sensible normalisation strategy, because the indicators are now on very different ranges.

In practice, if different normalisation strategies are selected, it is a good idea to keep the indicators on similar ranges, otherwise the effects will be very unequal in the aggregation step.

## Data frames and vectors

Normalising a data frame is very similar to normalising a coin, except the input is a data frame and output is also a data frame.

```{r}
mtcars_n <- Normalise(mtcars, default_specs = list(f_n = "n_dist2max"))

head(mtcars_n)
```

As with coins, columns can be normalised with individual specifications using the `indiv_spec` argument in exactly the same way as with a coin. Note that non-numeric columns are always ignored:

```{r}
Normalise(iris) |>
  head()
```

There is also a method for numeric vectors, although usually it is just as easy to call the underlying normalisation function directly.

```{r}
# example vector
x <- runif(10)

# normalise using distance to reference (5th data point)
x_norm <- Normalise(x, f_n = "n_dist2ref", f_n_para = list(iref = 5))

# view side by side
data.frame(x, x_norm)
```

## Purses

The purse method for `normalise()` is especially useful if you are working with multiple coins and panel data. This is because to make scores comparable from one time point to the next, it is usually a good idea to normalise indicators together rather than separately. For example, with the min-max method, indicators are typically normalised using the minimum and maximum over all time points of data, as opposed to having a separate max and min for each.

If indicators were normalised separately for each time point, then the highest scoring unit would get a score of 100 in time $t$ (assuming min-max between 0 and 100), but the highest scoring unit in time $t+1$ would *also* be assigned a score of 100. The underlying values of these two scores could be very different, but they would get

This means that the purse method for `normalise()` is a bit different from most other purse methods, because it doesn't independently apply the function to each coin, but takes the coins all together. This has the following implications:

1. Any normalisation function can be applied globally to all coins in a purse, ensuring comparability. BUT:
2. If normalisation is done globally, it is no longer possible to automatically regenerate coins in the purse (i.e. using `regenerate()`), because the coin is no longer self-contained: it needs to know the values of the other coins in the purse. Perhaps at some point I will add a dedicated method for regenerating entire purses, but we are not there yet.

Let's anyway illustrate with an example. We build the example purse first.

```{r}
purse <- build_example_purse(quietly = TRUE)
```

Normalising a purse works in exactly the same way as normalising a coin, except for the `global` argument. By default, `global = TRUE`, which means that the normalisation will be applied over all time points simultaneously, with the aim of making the index comparable. Here, we will apply the default min-max approach to all coins:

```{r}
purse <- Normalise(purse, dset = "Raw", global = TRUE)
```

Now let's examine the data set of the first coin. We'll see what the max and min of a few indicators is:

```{r}
# get normalised data of first coin in purse
x1 <- get_dset(purse$coin[[1]], dset = "Normalised")

# get min and max of first four indicators (exclude uCode col)
sapply(x1[2:5], min, na.rm = TRUE)
sapply(x1[2:5], max, na.rm = TRUE)
```

Here we see that the minimum values are zero, but the maximum values are *not* 100, because in other coins these indicators have higher values. To show that the global maximum is indeed 100, we can extract the whole normalised data set for all years and run the same check.

```{r}
# get entire normalised data set for all coins in one df
x1_global <- get_dset(purse, dset = "Normalised")

# get min and max of first four indicators (exclude Time and uCode cols)
sapply(x1_global[3:6], min, na.rm = TRUE)
sapply(x1_global[3:6], max, na.rm = TRUE)
```

And this confirms our expectations: that the global maximum and minimum are 0 and 100 respectively.

Any type of normalisation can be performed on a purse in this "global" mode. However, keep in mind what is going on. Simply put, when `global = TRUE` this is what happens:

1. The data sets from each coin are joined together into one using the `get_dset()` function.
2. Normalisation is applied to this global data set.
3. The global data set is then split back into the coins.

So if you specify to normalise by e.g. rank, ranks will be calculated for all time points. Therefore, consider carefully if this fits the intended meaning.

Normalisation can also be performed independently on each coin, by setting `global = FALSE`.

```{r}
purse <- Normalise(purse, dset = "Raw", global = FALSE)

# get normalised data of first coin in purse
x1 <- get_dset(purse$coin[[1]], dset = "Normalised")

# get min and max of first four indicators (exclude uCode col)
sapply(x1[2:5], min, na.rm = TRUE)
sapply(x1[2:5], max, na.rm = TRUE)
```

Now the normalised data set in each coin will have a min and max of 0 and 100 respectively, for each indicator.
